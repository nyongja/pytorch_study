{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 넘파이로 텐서 만들기(벡터와 행렬 만들기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy로 텐서 만드는 방법은 간단한데 [숫자, 숫자, 숫자]와 같은 형식으로 만들고 이를 np.array()로 감싸주면 됨. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 1D with Numpy\n",
    "\n",
    "Numpy로 1차원 벡터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "# 파이썬으로 설명하자면 List를 생성해서 np.array로 1차원 array로 변환. \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of t :  1\n",
      "Shape of t :  (7,)\n"
     ]
    }
   ],
   "source": [
    "print('Rank of t : ', t.ndim) # 1차원 벡터의 차원 출력\n",
    "print('Shape of t : ', t.shape) # 1차원 벡터의 크기 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .ndim : 몇 차원인지. 1차원은 벡터, 2차원은 행렬, 3차원은 3차원 텐서\n",
    "- .shape : 크기. (7,)는 (1, 7)을 의미. 다시 말해 (1 x 7)의 크기를 가지는 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1) Numpy 기초 이해하기\n",
    "이제 Numpy에서 각 벡터의 원소에 접근하는 방법을 알아보자.   \n",
    "Numpy에서 인덱스는 0부터 시작."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[0] t[1] t[-1] =  0.0 1.0 6.0\n"
     ]
    }
   ],
   "source": [
    "print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1]) # 인덱스를 통한 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "범위 지정으로도 원소 불러올 수 있음. => 슬라이싱(Slicing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 2D with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of t :  2\n",
      "Shape of t :  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Rank of t : ', t.ndim)\n",
    "print('Shape of t : ', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 파이토치 텐서 선언하기 (PyTorch Tensor Allocation)\n",
    "파이토치는 Numpy와 매우 유사. 하지만 더 낫다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 1D with PyTorch\n",
    "파이토치로 1차원 벡터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim()) # rank\n",
    "#print(t.shape()) \n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1차원 텐서이며, 원소는 7개.  \n",
    "인덱스와 슬라이싱으로 접근해보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n"
     ]
    }
   ],
   "source": [
    "print(t[0], t[1], t[-1])\n",
    "print(t[2:5], t[4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 2D with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                       ])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  5.,  8., 11.])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(t[:, 1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것ㅁ나 가져온다.\n",
    "print(t[:, 1].size()) # 위의 경우 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, 첫번재 차원을 전체 선택한 후, 그 중 두번째 차원의 1번 인덱스 값만을 가져온 경우를 말함.  \n",
    "다시 말해 텐서에서 두번재 열에 있는 모든 값을 가져온 것.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 브로드캐스팅(Broadcasting)\n",
    "두 행렬 A, B가 있을 때, 행렬의 덧셈과 뺄셈 시에는 두 행렬 A, B의 크기가 같아야 함.  \n",
    "또한 곱셈 시에는 A의 마지막 차원과 B의 첫번째 차원이 일치해야 함.  \n",
    "\n",
    "물론, 이런 규칙들이 있으나 딥 러닝에서는 불가피하게 크기가 다른 행렬 또는 텐서에 대해 사칙 연산을 해야할 상황이 생김.  \n",
    "이를 위해 파이토치에서는 자동으로 크기를 맞춰서 연살 할 수 있게 도와주는 **브로드캐스팅** 이라는 기능을 제공.  \n",
    "\n",
    "우선 같은 크기일 때 연산을 해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 크기가 다른 텐서들 간의 연산을 보자.  \n",
    "벡터와 슼라라가 덧셈 연산 시 수학적으로는 안되나 파이토치에서는 브로드캐스팅을 통해 이를 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# Vector + scala\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([3]) # -> [3, 3]\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m1의 크기는 (1, 2)이며 m2의 크기는 (1,)이다.  \n",
    "그런데 파이토치는 m2의 크기를 (1, 2)로 변경하여 연산을 수행한다.  \n",
    "\n",
    "이번에는 벡터 간 연산에서 브로드캐스팅이 적용되는 경우를 살펴보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 2 x 1 Vector + 1 x 2 Vector\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([[3], [4]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m1의 크기는 (1, 2), m1의 크기는 (2, 1).  \n",
    "그러나 파이토치는 두 벡터의 크기를 (2, 2)로 변경하여 수행하였음.  \n",
    "\n",
    "[1, 2]\n",
    "==> [[1, 2],\n",
    "     [1, 2]]\n",
    "[3]\n",
    "[4]\n",
    "==> [[3, 3],\n",
    "     [4, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "브로드캐스팅은 편리하나 자동으로 실행되는 기능이므로 매우 주의해야 함.  \n",
    "두 텐서의 크기가 달라 에러가 나면 사용자는 연산이 잘못되었음을 바로 알 수 있으나 브로드캐스팅은 자동으로 수행되므로 사용자는 나중에 원하는 결과가 나오지 않아도 어디서 문제가 발생하였는지 찾기 매우 어려움. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 자주 사용되는 기능들\n",
    "\n",
    "#### 1) 행렬 곱셈과 곱셈의 차이 (Matrix Multiplication Vs. Multiplication)\n",
    "행렬로 곱셈을 하는 방법은 크게 두 가지 있음.  \n",
    "바로 행렬 곱셈(.matmul)과 원소 별 곱셈(.mul)  \n",
    "\n",
    "pytorch tensor의 행렬 곱셈을 해보자.  \n",
    "이는 matmul()을 통해 수행!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1 :  torch.Size([2, 2])\n",
      "Shape of Matrix 2 :  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1, 2,], [3, 4]])\n",
    "m2 = torch.FloatTensor(([1], [2]))\n",
    "print('Shape of Matrix 1 : ', m1.shape)\n",
    "print('Shape of Matrix 2 : ', m2.shape)\n",
    "print(m1.matmul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬 곱셈이 아니라 element-wise 곱셈이라는 것도 존재.  \n",
    "이는 **동일한 크기의 행렬** 이 동일한 위치에 있는 원소끼리 곱하는 것을 읨.  \n",
    "\n",
    "아래는 서로 다른 크기의 행렬이 브로드캐스팅 된 후 element-wise 곱셈이 수행되는 것을 보여줌.  \n",
    "이는 * 또는 mul()을 통해 수행!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1 :  torch.Size([2, 2])\n",
      "Shape of Matrix 2 :  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1, 2,], [3, 4]])\n",
    "m2 = torch.FloatTensor(([1], [2]))\n",
    "print('Shape of Matrix 1 : ', m1.shape)\n",
    "print('Shape of Matrix 2 : ', m2.shape)\n",
    "print(m1 * m2)\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 평균(Mean)\n",
    "이는 Numpy에서의 사용법과 매우 유사.  \n",
    "우선 1차원인 벡터를 선언하여 .mean()을 사용하여 원소의 평균을 구함.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([1, 2])\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 2차원 행렬을 선언하여 .mean()을 사용해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 1, 2, 3, 4 원소의 평균인 **2.5**  \n",
    "이번에는 dim, 즉 차원(dimension)을 인자로 주는 경우를 살펴보자. \n",
    "\n",
    "dim = 0 이라는 것은 첫 번재 차원을 의미.  \n",
    "행렬에서 첫번째 차원은 '행'을 의미함.  \n",
    "**그리고 인자로 dim을 준다면 해당 차원을 제거한다는 의미가 됨.**   \n",
    "다시 말해 행렬에서 '열'만을 남기겠다는 의미.  \n",
    "\n",
    "기존 행렬의 크기는 (2, 2)였지만 이를 수행하면 열의 차원만 보존되면서 (1, 2)가 됨니다.  \n",
    "이는 (2,)와 같으며 벡터 임.  \n",
    "열의 차원을 보존하면서 평균을 구하면 아래와 같이 연산함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean(dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean(dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인자로 dim = 1을 주면 두 번재 차원은 제거된, 즉 열이 제거된 텐서가 나옴.  \n",
    "\n",
    "열의 차원이 제거되어야 하므로 (2, 2)크기에서 (2, 1)의 크기가 됨.  \n",
    "이번에는 1 과 3의 평균을 구하고 3과 4의 평균을 구함\n",
    "\n",
    "하지만 (2 X 1)은 결국 1차원 이므로 (1 X 2)와 같이 표현되면서 위와 같이 [1.5, 3.5]로 출력됨.  \n",
    "이번에는 dim = -1을 주는 경우르 보자.  \n",
    "이는 마지막 차원을 제거한다는 의미이고, 결국 열의 차원을 제거한다는 의미와 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean(dim = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 덧셈(Sum)\n",
    "덧셈은 평균과 연산 방법이나 인자가 의미하는 바가 정확히 동일함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "print(t.sum()) # 단순히 원소 전체의 덧셈 수행\n",
    "print(t.sum(dim = 0)) # 행을 제거\n",
    "print(t.sum(dim = 1)) # 열을 제거\n",
    "print(t.sum(dim = -1)) # 열을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 최대(Max)와 아그맥스(ArgMax)\n",
    "최대(Max)는 원소의 최대값을 리턴하고, 아그맥스(ArgMax)는 최대값을 가진 인덱스를 리턴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "print(t.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 dim = 0 을 주겠습니다.  \n",
    "이는 위와 같이 첫번째 차원을 제거한다는 의미."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 [1, 1]은 max에 dim 인자를 줄 시 argmax 값도 함게 리턴하는 특징이 있기 때문에 함께 나타난 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 두 개를 함께 리턴받는 것이 아니라 max 또는 argmax만 리턴받고 싶으면 다음과 같이 리턴값에도 인덱스를 부여하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max :  tensor([3., 4.])\n",
      "Argmax :  tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print('Max : ', t.max(dim = 0)[0])\n",
    "print('Argmax : ', t.max(dim = 0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 dim = 1로 인자를 주었을 때와 dim = -1로 인자를 주었을 때를 살펴보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim = 1))\n",
    "print(t.max(dim = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터사이언스 분야 한정으로 3차원 이상의 tensor는 그냥 다차원 행렬 또는 배열로 간주할 수 있음. 또한 주로 3차원 이상을 텐서라고 하긴 하지만, 1차원 벡터나 2차원 행렬도 텐서라고 표현하기도 함. 1차원 벡터 = 1차원 텐서, 2차원 벡터 = 2차원 텐서 ...  \n",
    "- 훈련 데이터 하나의 크기를 256이라 하면 [3, 5, 1, 2, 3, ...]이런 숫자들의 나열이 256의 길이로 있다고 상상하면 됨. 다시 말해 훈련 데이터 하나 = 벡터의 차원은 256. 만약 이런 훈련 데이터의 개수가 3000개라고 하면 전체 훈련 데이터의 크기는 3,000 X 256. 3,000개를 1개씩 꺼내서 처리하는 것도 가능하지만 컴퓨터는 훈련 데이터를 하나씩 처리하는 것보다 보통 덩어리로 처리. 3,000개 중에서 64개씩 꺼내서 처리한다고 하면 이때 batch size를 64라고 함. 그렇다면 컴퓨터가 한 번에 처리하는 2D 텐서의 크기는 batch size X dim = 64 * 256\n",
    "- 텐서의 크기(shape)을 표현할 때에는 ,(comma)를 쓰기도 하고 x(곱하기)를 쓰기도 함. 예를 들어 2행 3열의 2D텐서를 표현할 때 (2, 3)라고 하기도 하고, (2 X 3)이라고 하기도 함. (5, )의 형식은 (1 X 5)를 의미.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
